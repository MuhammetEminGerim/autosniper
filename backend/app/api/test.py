from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks
from sqlalchemy.orm import Session
from app.core.database import get_db, SessionLocal
from app.api.dependencies import get_current_user, check_rate_limit
from app.models.user import User
from app.models.listing import Listing
from app.models.filter import Filter
from app.services.filter_matcher import FilterMatcher
from app.services.websocket.manager import manager
from app.services.scraper.scraper import ArabaComScraper
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

router = APIRouter()

@router.post("/test/add-listing")
async def add_test_listing(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Test iÃ§in Ã¶rnek ilan ekle ve filtreleri kontrol et
    """
    # Test ilanÄ± oluÅŸtur
    test_listing = Listing(
        source_url=f"https://test-araba.com/ilan-{datetime.now().timestamp()}",
        title="Test AraÃ§ - Audi A3 2018 Dizel Otomatik",
        price=850000.0,
        year=2018,
        brand="Audi",
        model="A3",
        fuel_type="dizel",
        transmission="otomatik",
        city="Ankara",
        description="Test iÃ§in eklenen Ã¶rnek ilan. Bu ilan filtrelerinize uyuyorsa bildirim alacaksÄ±nÄ±z.",
        images=["https://via.placeholder.com/600x400?text=Test+AraÃ§"],
        is_new=True
    )
    
    db.add(test_listing)
    db.flush()  # ID'yi almak iÃ§in
    
    # KullanÄ±cÄ±nÄ±n aktif filtrelerini al
    user_filters = db.query(Filter).filter(
        Filter.user_id == current_user.id,
        Filter.is_active == True
    ).all()
    
    matching_filters = []
    
    # Filtreleri kontrol et
    for filter_obj in user_filters:
        if FilterMatcher.matches(test_listing, filter_obj):
            matching_filters.append(filter_obj)
            
            # WebSocket bildirimi gÃ¶nder
            try:
                await manager.send_personal_message({
                    "type": "new_listing",
                    "message": f"ğŸ¯ Filtrenize uyan yeni ilan bulundu: {test_listing.title}",
                    "listing": {
                        "id": test_listing.id,
                        "title": test_listing.title,
                        "price": test_listing.price,
                        "source_url": test_listing.source_url,
                        "year": test_listing.year,
                        "brand": test_listing.brand,
                        "model": test_listing.model,
                        "city": test_listing.city,
                    },
                    "filter_id": filter_obj.id,
                    "filter_name": filter_obj.name,
                }, current_user.id)
                
                logger.info(f"Test ilanÄ± iÃ§in bildirim gÃ¶nderildi: Filtre {filter_obj.name}")
            except Exception as e:
                logger.error(f"Bildirim gÃ¶nderilirken hata: {e}")
    
    db.commit()
    
    return {
        "message": "Test ilanÄ± eklendi",
        "listing": {
            "id": test_listing.id,
            "title": test_listing.title,
            "price": test_listing.price,
            "year": test_listing.year,
            "brand": test_listing.brand,
            "model": test_listing.model,
            "city": test_listing.city,
        },
        "total_filters": len(user_filters),
        "matching_filters": [
            {
                "id": f.id,
                "name": f.name,
                "criteria": f.criteria
            } for f in matching_filters
        ],
        "notification_sent": len(matching_filters) > 0
    }

@router.post("/test/add-custom-listing")
async def add_custom_test_listing(
    title: str = "Test AraÃ§",
    price: float = 500000.0,
    year: int = 2020,
    brand: str = "Audi",
    model: str = "A3",
    fuel_type: str = "dizel",
    transmission: str = "otomatik",
    city: str = "Ankara",
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    Ã–zelleÅŸtirilmiÅŸ test ilanÄ± ekle
    """
    test_listing = Listing(
        source_url=f"https://test-araba.com/ilan-{datetime.now().timestamp()}",
        title=title,
        price=price,
        year=year,
        brand=brand,
        model=model,
        fuel_type=fuel_type,
        transmission=transmission,
        city=city,
        description="Test iÃ§in eklenen Ã¶rnek ilan.",
        images=["https://via.placeholder.com/600x400?text=Test+AraÃ§"],
        is_new=True
    )
    
    db.add(test_listing)
    db.flush()
    
    # Filtreleri kontrol et
    user_filters = db.query(Filter).filter(
        Filter.user_id == current_user.id,
        Filter.is_active == True
    ).all()
    
    matching_filters = []
    for filter_obj in user_filters:
        if FilterMatcher.matches(test_listing, filter_obj):
            matching_filters.append(filter_obj)
            try:
                await manager.send_personal_message({
                    "type": "new_listing",
                    "message": f"ğŸ¯ Test ilanÄ±: {test_listing.title}",
                    "listing": {
                        "id": test_listing.id,
                        "title": test_listing.title,
                        "price": test_listing.price,
                        "source_url": test_listing.source_url,
                    },
                    "filter_id": filter_obj.id,
                    "filter_name": filter_obj.name,
                }, current_user.id)
            except Exception as e:
                logger.error(f"Bildirim hatasÄ±: {e}")
    
    db.commit()
    
    return {
        "message": "Ã–zel test ilanÄ± eklendi",
        "listing_id": test_listing.id,
        "matches_filters": len(matching_filters),
        "matching_filter_names": [f.name for f in matching_filters]
    }

@router.post("/scrape")
async def scrape_with_criteria(
    criteria: dict = {},
    current_user: User = Depends(check_rate_limit),
    db: Session = Depends(get_db)
):
    """
    Kriterlere gÃ¶re gerÃ§ek siteden ilanlarÄ± Ã§ek (Arama sayfasÄ± iÃ§in)
    """
    import sys
    print("=" * 50, file=sys.stderr)
    print("ARAMA BAÅLADI", file=sys.stderr)
    print(f"Kriterler: {criteria}", file=sys.stderr)
    print("=" * 50, file=sys.stderr)
    
    scraper = None
    try:
        scraper = ArabaComScraper(db)
        await scraper.init_browser()
        
        # Kriterleri scraper'a gÃ¶nder
        search_params = criteria.get("criteria", criteria) if criteria else {}
        listings = await scraper.scrape_listings(search_params)
        
        new_count = await scraper.save_new_listings(listings)
        
        await scraper.close_browser()
        
        return {
            "message": "Arama tamamlandÄ±",
            "total_scraped": len(listings),
            "new_listings_added": new_count
        }
    except Exception as e:
        print(f"HATA: {e}", file=sys.stderr)
        if scraper:
            try:
                await scraper.close_browser()
            except:
                pass
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Arama baÅŸarÄ±sÄ±z: {str(e)}"
        )


@router.post("/scrape-real-listings")
async def scrape_real_listings(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    GerÃ§ek siteden ilanlarÄ± Ã§ek ve sisteme ekle (TEST Ä°Ã‡Ä°N - eski endpoint)
    """
    import sys
    print("=" * 50, file=sys.stderr)
    print("SCRAPER TEST BAÅLADI", file=sys.stderr)
    print("=" * 50, file=sys.stderr)
    
    scraper = None
    try:
        print("1. Scraper baÅŸlatÄ±lÄ±yor...", file=sys.stderr)
        logger.info("=" * 50)
        logger.info("SCRAPER TEST BAÅLADI")
        logger.info("=" * 50)
        logger.info("Scraper baÅŸlatÄ±lÄ±yor...")
        
        # Scraper'Ä± baÅŸlat
        scraper = ArabaComScraper(db)
        print("2. TarayÄ±cÄ± baÅŸlatÄ±lÄ±yor...", file=sys.stderr)
        logger.info("TarayÄ±cÄ± baÅŸlatÄ±lÄ±yor...")
        await scraper.init_browser()
        print("3. TarayÄ±cÄ± baÅŸlatÄ±ldÄ±!", file=sys.stderr)
        logger.info("TarayÄ±cÄ± baÅŸlatÄ±ldÄ±")
        
        # Ä°lanlarÄ± Ã§ek (Ã¶rnek parametrelerle)
        print("4. Ä°lanlar Ã§ekiliyor...", file=sys.stderr)
        logger.info("Ä°lanlar Ã§ekiliyor...")
        listings = await scraper.scrape_listings({
            "brand": None,  # TÃ¼m markalar
            "city": None    # TÃ¼m ÅŸehirler
        })
        print(f"5. Toplam {len(listings)} ilan bulundu", file=sys.stderr)
        logger.info(f"Toplam {len(listings)} ilan bulundu")
        
        # Bulunan ilanlarÄ± logla
        for i, listing in enumerate(listings[:5]):
            print(f"   Ä°lan {i+1}: {listing.get('title', 'N/A')[:50]} - {listing.get('price', 0)} TL", file=sys.stderr)
            logger.info(f"Ä°lan {i+1}: {listing.get('title', 'N/A')} - {listing.get('price', 0)} TL")
        
        # Yeni ilanlarÄ± kaydet ve filtreleri kontrol et
        print("6. Ä°lanlar kaydediliyor...", file=sys.stderr)
        logger.info("Ä°lanlar kaydediliyor...")
        new_count = await scraper.save_new_listings(listings)
        print(f"7. {new_count} yeni ilan kaydedildi", file=sys.stderr)
        logger.info(f"{new_count} yeni ilan kaydedildi")
        
        if scraper:
            print("8. TarayÄ±cÄ± kapatÄ±lÄ±yor...", file=sys.stderr)
            logger.info("TarayÄ±cÄ± kapatÄ±lÄ±yor...")
            await scraper.close_browser()
            print("9. TamamlandÄ±!", file=sys.stderr)
            logger.info("TamamlandÄ±!")
        
        return {
            "message": "Scraping tamamlandÄ±",
            "total_scraped": len(listings),
            "new_listings_added": new_count,
            "listings": [
                {
                    "title": l.get("title", "Bilinmeyen"),
                    "price": l.get("price", 0),
                    "brand": l.get("brand"),
                    "model": l.get("model"),
                    "city": l.get("city"),
                } for l in listings[:10]  # Ä°lk 10'unu gÃ¶ster
            ]
        }
    except Exception as e:
        print(f"HATA: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc(file=sys.stderr)
        logger.error(f"Scraping hatasÄ±: {e}", exc_info=True)
        if scraper:
            try:
                await scraper.close_browser()
            except:
                pass
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Scraping baÅŸarÄ±sÄ±z: {str(e)}"
        )

